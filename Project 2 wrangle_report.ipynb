{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wragle_report\n",
    "* Create a **300-600 word written report** called \"wrangle_report.pdf\" or \"wrangle_report.html\" that briefly describes your wrangling efforts. This is to be framed as an internal document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling Project - Babajide Tobiloba\n",
    "\n",
    "The goal of this project is the practice the data wrangling skills obtained in the Data Wrangling Course of the Udacity Data Analyst Nanodegree program. The data that was wrangled is the tweet archive of a funny Twitter account that rates people’s dogs, known as We Rate Dogs (@Dog_rates). This data was gathered from multiple sources, cleaned, and then, used for analysis.\n",
    "This report documents my data wrangling journey."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Gathering\n",
    "\n",
    "The data for this project came from three different datasets, which were obtained as follows:\n",
    "\n",
    "`twitter_archive_enhanced.csv`: This file was provided by Udacity and after downloading it, I loaded it into my workspace using the pandas function `read_csv()`.\n",
    "\n",
    "`image_predictions.tsv`: I added Python requests and os libraries. I obtained the data via its url using the requests library's `get()` function and saved it in a response variable. I wrote this response into a tsv file called image_predictions. Then, I loaded the tsv file using the pandas function `read_csv()`.\n",
    "\n",
    "`tweet_json.txt`: Unfortunately, my developer account was not approved by Twitter in time for the project, and so I was not able to write any of my own code for this section of the data gathering. I proceeded to use the code provided by Udacity but encountered errors while doing so. I eventually used the json file provided by Udacity for the analysis. Using the with open function, I read the file as a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Assessment\n",
    "\n",
    "I then proceeded to assess the data after obtaining the three tables. Two assessment methods were employed:\n",
    "__Visual assessment__: I printed out each of the three dataframes separately and examined thoroughly.\n",
    "\n",
    "__Programmatic assessment__: Using various python and pandas methods and functions, including `.info()`, `.duplicated()`, `.isnull()`, `.describe()`, `.unique()`, I conducted various programmatic assessments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "Now, I had to clean the data by following the Define, Code, Test format. First, I created copies of the data frames to be cleaned using the `.copy()` function.\n",
    "I first documented the issues I observed in the data frames by visual and programmatic assessment:\n",
    "__Quality issues__\n",
    "\n",
    "a. twitter_archive dataframe:\n",
    "\n",
    "1.\tthere are some retweeted tweets as shown in the `retweeted_status_id column`\n",
    "2.\t`timestamp` is in string format instead of datetime\n",
    "3.\t59 missing values in `expanded_urls` column\n",
    "4.\t`tweet_id` column is in integer format instead of string\n",
    "5.\t`rating_denominator` column should only have values of 10\n",
    "\n",
    "b. image_predictions dataframe:\n",
    "\n",
    "1.\tincorrect dog breed name \"orange\" in `p1` column\n",
    "2.\tincorrect dog breed name \"spatula\" in `p3` column\n",
    "3.\t`tweet_id` column is in integer format instead of string\n",
    "\n",
    "c. json_twitter_archive dataframe:\n",
    "\n",
    "1.\t143 missing values in `possibly_sensitive` and `possibly_sensitive_appealable` columns\n",
    "2.\t281 missing values in `extended_entities` column\n",
    "\n",
    "__Tidiness Issues__\n",
    "\n",
    "1. `doggo`, `floofer`, `pupper`, `puppo` columns need to be combined into one column \n",
    "\n",
    "2. inconsistent naming of `id_str` in `json_twitter_archive`\n",
    "\n",
    "3. certain columns are not needed for analysis and visualization\n",
    "\n",
    "4. The three dataframes need to be combined into one table with only the relevant columns\n",
    "\n",
    "Then, I proceeded to clean them:\n",
    "\n",
    "- I dropped rows that have values in `retweeted_status_id` column in `twitter_archive` dataframe\n",
    "- I converted `timestamp` to datetime format from string format in `twitter_archive` dataframe\n",
    "- I deleted rows with missing values in expanded_urls column in `twitter_archive` dataframe\n",
    "- I converted `tweet_id` column to string format instead of integer format in `twitter_archive` dataframe\n",
    "- I dropped rows where values in `rating_denominator` column are greater than 10 in `twitter_archive` dataframe\n",
    "- I dropped the row with incorrect dog breed name \"orange\" in `p1` column in `image_predictions` dataframe\n",
    "- I dropped row with incorrect dog breed name \"spatula\" in `p3` column in `image_predictions` dataframe\n",
    "- I converted `tweet_id` column to string format instead of integer format in `image_predictions` dataframe\n",
    "- I deleted rows with missing values in `possibly_sensitive` column in `json_twitter_archive` dataframe\n",
    "- I deleted rows with missing values in extended_entities column `json_twitter_archive` dataframe\n",
    "- I combined `doggo`, `floofer`, `pupper`, `puppo` into one column called `stage` in `twitter_archive` dataframe\n",
    "- I renamed `id_str` in json_twitter_archive to `tweet_id`\n",
    "- I dropped unnecessary columns in the three dataframes\n",
    "- I merged the three dataframes into one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Storing\n",
    "\n",
    "I then stored the merged data as a csv file called `twitter_archive_master.csv` using the `to_csv()` function to prepare it for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "This project was challenging, especially the data gathering section of it. I encountered challenge while trying to query Twitter’s API and this stalled me for some time but in the end, I was able to proceed and eventually carry out the data wrangling. I hope to work on more data wrangling projects to become an expert data wrangler.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
